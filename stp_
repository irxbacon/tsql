import org.apache.hadoop.conf.Configurationimport org.apache.spark._import org.apache.spark.sql._import org.apache.spark.sql.functions._import org.apache.spark.sql.hive._import org.apache.hadoop.hbase.HBaseConfiguration
val sql_context = new HiveContext(sc)val config = HBaseConfiguration.create()
var df = Seq((1, 1211, "2017-11-01", 1, 0, 96, 150),(2, 1211, "2017-11-01", 1, 1, 23, 66),(3, 1211, "2017-11-01", 1, 1, 75, 120),(4, 1211, "2017-11-01", 1, 1, 64, 90),(5, 1211, "2017-11-01", 1, 2, 43, 94),(6, 1211, "2017-11-01", 1, 2, 74, 103),(7, 1211, "2017-11-01", 1, 2, 23, 52),(8, 1211, "2017-11-01", 1, 2, 47, 66),(9, 1211, "2017-11-01", 1, 3, 79, 163),(10, 1211, "2017-11-01", 1, 3, 90, 180),(11, 1211, "2017-11-01", 1, 3, 60, 126),(12, 1211, "2017-11-01", 1, 3, 18, 97),(13, 1211, "2017-11-01", 1, 4, 57, 138),(14, 1211, "2017-11-01", 1, 4, 69, 77),(15, 1211, "2017-11-01", 1, 4, 9, 102),(16, 1211, "2017-11-01", 1, 4, 93, 124),(17, 1211, "2017-11-01", 1, 5, 46, 95),(18, 1211, "2017-11-01", 1, 5, 40, 72),(19, 1211, "2017-11-01", 1, 6, 66, 153),(20, 1211, "2017-11-01", 1, 6, 61, 124),(21, 1211, "2017-11-01", 1, 19, 18, 62),(22, 1211, "2017-11-01", 1, 19, 43, 49),(23, 1211, "2017-11-01", 1, 19, 70, 125),(24, 1211, "2017-11-01", 1, 20, 16, 111),(25, 1211, "2017-11-01", 1, 10, 30, 56),(26, 1211, "2017-11-01", 1, 10, 17, 54),(27, 1211, "2017-11-01", 1, 11, 68, 138),(28, 1211, "2017-11-01", 1, 11, 85, 174),(29, 1211, "2017-11-01", 1, 11, 4, 31),(30, 1211, "2017-11-01", 1, 11, 11, 43))toDF("hierarchy_id", "metric", "partition_date", "country_id", "parent_id", "numerator", "denominator")
  def recurHier(config: Configuration, hierDF: DataFrame, startParent: Int) : DataFrame = {
    var children = hierDF.filter(col("parent_id")=== startParent).collect()    var currentRow = hierDF.filter(col("hierarchy_id")=== startParent)
    var grandParent : Int = -1
    if(currentRow.select("parent_id").count() > 0 ){      grandParent = currentRow.select("parent_id").collect()(0).getInt(0)    }    var returnDF = sql_context.createDataFrame(sc.emptyRDD[Row], hierDF.schema)    children.foreach(child => returnDF = returnDF.unionAll(recurHier(config, hierDF, child.getInt(0))))
    var aggDF = returnDF      .select(col("hierarchy_id"), col("metric"), col("partition_date"), col("country_id"), lit( grandParent) as "parent_id", col("numerator"), col("denominator"))      .unionAll(currentRow)
    if(aggDF.count() > 0){//      tmpDF.show()      aggDF = aggDF        .groupBy(lit(startParent) as "hierarchy_id", col("metric"), col("partition_date"), col("country_id"), col("parent_id"))        .agg(sum("numerator") as "numerator", sum("denominator") as "denominator")    }    returnDF = returnDF.unionAll(aggDF)//    println("returnDF agg")//    returnDF.//      select(functions.concat_ws("_" , col("metric"), col("partition_date"), col("hierarchy_id"), col("country_id") ) as "rowKey"//        , col("metric"), col("hierarchy_id"), col("parent_id"), col("partition_date"), col("country_id"), col("numerator"), col("denominator")).show//    helpClass.writeToHBase(config, "hierarchy_agg", "ha"//      , returnDF.select(functions.concat_ws("_" , col("metric"), col("partition_date"), col("hierarchy_id"), col("country_id") ) as "rowKey", col("metric"), col("hierarchy_id"),col("parent_id"), col("partition_date"), col("country_id"), col("numerator"), col("denominator"))//    )    returnDF.cache()    returnDF  }
